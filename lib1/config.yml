---
image_input: 'images'       # input image dir
movie_input: 'input.mp4'    # mp4 or avi. Movie file.
#camera_input: 0            # USB Webcam on PC
camera_input: 1             # USB Webcam on TX2
## Input Must be OpenCV readable
## Onboard camera on Xavier (with TX2 onboard camera)
#camera_input: "nvarguscamerasrc ! video/x-raw(memory:NVMM), width=1280, height=720,format=NV12, framerate=120/1 ! nvvidconv ! video/x-raw,format=I420 ! videoflip method=rotate-180 ! appsink"
## Onboard camera on TX2 ### (need: apt-get install libxine2)
#camera_input: "nvcamerasrc ! video/x-raw(memory:NVMM), width=(int)1280, height=(int)720, format=(string)I420, framerate=(fraction)30/1 ! nvvidconv flip-method=0 ! video/x-raw, format=(string)BGRx ! videoconvert ! video/x-raw, format=(string)BGR ! appsink"
## Onboard or external RTSP feed
#camera_input: "rtspsrc location=rtsp://127.0.0.1:8554/test latency=500 ! rtph264depay ! h264parse ! omxh264dec ! nvvidconv ! video/x-raw, width=(int)1280, height=(int)720, format=(string)BGRx ! videoconvert ! appsink"

force_gpu_compatible: False # If True with visualize False, speed up. Forces all CPU tensors to be allocated with Cuda pinned memory.
save_to_file: False         # movie or camera: ./output_movie/output_unixtime.avi. Save it in avi format to prevent compression degradation. Requires a lot of disk space.
                            # image: ./output_image/PATH_TO_FILE. Save it in image file.
visualize: True             # True: Show result image. False: Without image show.
vis_worker: False           # True: Visualization run on process. (With visuzalize:True)
max_vis_fps: 0              # >=1: Limit of show fps. 0: No limit - means try to spend full machine power for visualization. (With visualize:True.)
vis_text: True              # Display fps on result image. (With visualize:True.)
max_frames: 5000            # >=1: Quit when frames done. 0: no exit. (With visualize:False)
width: 600                  # Camera width.
height: 600                 # Camera height.
fps_interval: 5             # FPS console out interval and FPS stream length.
det_interval: 100           # intervall [frames] to print detections to console
det_th: 0.5                 # detection threshold for det_intervall
worker_threads: 4           # parallel detection for Mask R-CNN.
split_model: True           # Splits Model into a GPU and CPU session for SSD/Faster R-CNN.
log_device: False           # Logs GPU / CPU device placement
allow_memory_growth: True   # limits memory allocation to the actual needs
debug_mode: False           # Show FPS spike value
label_path: 'models/labels/mscoco_label_map.pbtxt' # default: mscoco 90 classes
split_shape: 1917           # 1917, 3000, 3309, 5118, 7326, 51150. ExpandDims_1's shape.
num_classes: 90

# model_type supports 'nms_v0', 'nms_v1', 'nms_v2', 'trt_v1', 'mask_v1', 'faster_v2', 'deeplab_v3'
########################################
# model_type: 'nms_v0'
# split_shape: 1917
# This is for 11 June 2017 model
########################################
# KNOWN MODELS
# ssd_mobilenet_v1_coco_11_06_2017
#model_type: 'nms_v0'
#model_path: 'models/ssd_mobilenet_v1_coco_11_06_2017/frozen_inference_graph.pb'


########################################
# model_type: 'nms_v1'
# split_shape: 1917
# This is for 2017_11_17 models
########################################
# KNOWN MODELS
# ssd_mobilenet_v1_coco_2017_11_17
# ssd_inception_v2_coco_2017_11_17
#model_type: 'nms_v1'
#model_path: 'models/ssd_mobilenet_v1_coco_2017_11_17/frozen_inference_graph.pb'


########################################
# model_type: 'nms_v1'
# split_shape: 1917
# This is for my own data
########################################
# ssd_mobilenet_v1_2017_11_17 own data
#model_type: 'nms_v1'
#model_path: 'models/ssd_mobilenet_v1_roadsign_2017_11_17/roadsign_frozen_inference_graph_v1_204k.pb'
#label_path: 'models/ssd_mobilenet_v1_roadsign_2017_11_17/roadsign_label_map.pbtxt'
#num_classes: 4


########################################
# model_type: 'nms_v2'
# split_shape: 1917
# This is for 2018_xx_xx models
########################################
# KNOWN MODELS
# ssd_mobilenet_v1_coco_2018_01_28
# ssd_mobilenet_v2_coco_2018_03_29
# ssdlite_mobilenet_v2_coco_2018_05_09
# ssd_inception_v2_coco_2018_01_28
# ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_03
# ssd_mobilenet_v1_0.75_depth_300x300_coco14_sync_2018_07_03
# ssd_mobilenet_v1_0.75_depth_quantized_300x300_coco14_sync_2018_07_03
model_type: 'nms_v2'
model_path: 'models/ssd_mobilenet_v1_coco_2018_01_28/frozen_inference_graph.pb'


########################################
# model_type: 'nms_v2'
# split_shape: 51150
########################################
# KNOWN MODELS
# ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03
# ssd_mobilenet_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03
#model_type: 'nms_v2'
#model_path: 'models/ssd_resnet50_v1_fpn_shared_box_predictor_640x640_coco14_sync_2018_07_03/frozen_inference_graph.pb'
#split_shape: 51150


########################################
# model_type: 'nms_v2'
# split_shape: 3000
########################################
# KNOWN MODELS
# ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync_2018_07_03
#model_type: 'nms_v2'
#model_path: 'models/ssd_mobilenet_v1_ppn_shared_box_predictor_300x300_coco14_sync_2018_07_03/frozen_inference_graph.pb'
#split_shape: 3000


########################################
# model_type: 'faster_v2'
########################################
# KNOWN MODELS
# faster_rcnn_inception_v2_coco_2018_01_28
# faster_rcnn_resnet50_coco_2018_01_28
# faster_rcnn_resnet101_coco_2018_01_28
# faster_rcnn_inception_resnet_v2_atrous_coco_2018_01_28
#model_type: 'faster_v2'
#model_path: 'models/faster_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb'


########################################
# model_type: 'trt_v1'
# split_shape: 1917
# This is for Tensorflow/TensorRT
########################################
# KNOWN MODELS
# ssd_mobilenet_v1_coco_2017_11_17
# ssd_inception_v2_coco_2017_11_17
# ssd_mobilenet_v1_coco_2018_01_28
# ssd_mobilenet_v2_coco_2018_03_29
# ssdlite_mobilenet_v2_coco_2018_05_09
# ssd_inception_v2_coco_2018_01_28
# NEW DIR will be created.
# ./data/ - download model. it has checkpoint.
# ./logs/ - Graph diagram for tensorboard.
# NEW FROZEN FILE will be created.
# ./data/ssd_mobilenet_v1_coco_2017_11_17_trt_FP16.pbrun_stream.py
# PRECISION MODEL
# https://devtalk.nvidia.com/default/topic/1023708/gpu-accelerated-libraries/fp16-support-on-gtx-1060-and-1080/
# FP16 is Tesla P100, Quadro GP100, and Jetson TX1/TX2.
# INT8 is cc6.1 or cc7.0
# FP32 for other devices.

#model_type: 'trt_v1'
#precision_model: 'FP32'     # 'FP32', 'FP16', 'INT8'
#model: 'ssdlite_mobilenet_v2_coco_2018_05_09'


########################################
# model_type: 'mask_v1'
# split_model: False
# split_model can be True, but it's slow yet.
########################################
# KNOWN MODELS
# mask_rcnn_inception_resnet_v2_atrous_coco_2018_01_28
# mask_rcnn_inception_v2_coco_2018_01_28
# mask_rcnn_resnet101_atrous_coco_2018_01_28
# mask_rcnn_resnet50_atrous_coco_2018_01_28
#model_type: 'mask_v1'
#model_path: 'models/mask_rcnn_inception_v2_coco_2018_01_28/frozen_inference_graph.pb'
#worker_threads: 4


########################################
# model_type: 'deeplab_v3'
# split_model: False
# split_model can be True, but it's slow yet.
########################################
# KNOWN MODELS
# deeplabv3_mnv2_pascal_train_aug_2018_01_29
# deeplabv3_mnv2_pascal_trainval_2018_01_29
# deeplabv3_pascal_train_aug_2018_01_04
# deeplabv3_pascal_trainval_2018_01_04
#model_type: 'deeplab_v3'
#model_path: 'models/deeplabv3_mnv2_pascal_train_aug/frozen_inference_graph.pb'
